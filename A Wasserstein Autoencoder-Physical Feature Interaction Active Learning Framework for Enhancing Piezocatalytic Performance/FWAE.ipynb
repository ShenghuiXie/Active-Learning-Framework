{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /home/shenghui/anaconda3/envs/shenghui/lib/python3.12/site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /home/shenghui/anaconda3/envs/shenghui/lib/python3.12/site-packages (from opencv-python) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.patches import Ellipse\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "sns.set(color_codes=True)\n",
    "!pip install bayesian-optimization\n",
    "!pip install bayesian-optimization\n",
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureDataset(Dataset):\n",
    "    '''\n",
    "    Args: \n",
    "    - x is a 2D numpy array [x_size, x_features] (e.g. compositions)\n",
    "    - y is the target (e.g. property to predict)\n",
    "    - descriptors are additional features for correlation calculation (e.g. physical properties)\n",
    "    '''\n",
    "    def __init__(self, x, y, descriptors=None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.descriptors = descriptors if descriptors is not None else x  \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.descriptors is not None:\n",
    "            return torch.FloatTensor(self.x[idx]), torch.FloatTensor(self.y[idx]), torch.FloatTensor(self.descriptors[idx])\n",
    "        return torch.FloatTensor(self.x[idx]), torch.FloatTensor(self.y[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "class FWAE(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(FWAE, self).__init__()\n",
    "        self.input_size = input_size  \n",
    "\n",
    "        # encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "                        nn.Linear(self.input_size,256),\n",
    "                        nn.LayerNorm(256),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(256, 128),\n",
    "                        nn.LayerNorm(128),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear (128, 64),\n",
    "                        nn.LayerNorm(64),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(64, 2),\n",
    "                        )\n",
    "\n",
    "        # decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "                        nn.Linear(2, 64),\n",
    "                        nn.LayerNorm(64),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(64, 128),\n",
    "                        nn.LayerNorm(128),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(128,256),\n",
    "                        nn.LayerNorm(256),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(256, self.input_size),\n",
    "                        nn.Softmax(dim=1), \n",
    "                        )\n",
    "        self.apply(weights_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self._encode(x)\n",
    "        x_recon = self._decode(z)\n",
    "        return x_recon, z\n",
    "    \n",
    "    def _encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def _decode(self, z):\n",
    "        return self.decoder(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_seeds(seed): \n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed) \n",
    "    np.random.seed(seed) \n",
    "    random.seed(seed)  \n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def get_latents(model, dataset): \n",
    "    model.to(device).eval() \n",
    "    latents = []\n",
    "    with torch.no_grad(): \n",
    "        dataloader = DataLoader(dataset, batch_size=128, shuffle=False)\n",
    "        for i, data in enumerate(dataloader):\n",
    "            x = data[0].to(device)\n",
    "            recon_x, z = model(x)\n",
    "            \n",
    "            latents.append(z.detach().cpu().numpy())\n",
    "    return np.concatenate(latents,axis=0)\n",
    "\n",
    "def imq_kernel(X: torch.Tensor, Y: torch.Tensor, h_dim: int): \n",
    "    batch_size = X.size(0)\n",
    "\n",
    "    norms_x = X.pow(2).sum(1, keepdim=True)  \n",
    "    prods_x = torch.mm(X, X.t()).to(device)  \n",
    "    dists_x = norms_x + norms_x.t() - 2 * prods_x \n",
    "\n",
    "    norms_y = Y.pow(2).sum(1, keepdim=True).to(device)  \n",
    "    prods_y = torch.mm(Y, Y.t()).to(device)  \n",
    "    dists_y = norms_y + norms_y.t() - 2 * prods_y\n",
    "\n",
    "    dot_prd = torch.mm(X, Y.t())\n",
    "    dists_c = norms_x + norms_y.t() - 2 * dot_prd\n",
    "\n",
    "    stats = 0\n",
    "    for scale in [.1, .2, .5, 1., 2., 5., 10.]: \n",
    "        C = 2 * h_dim * 1.0 * scale\n",
    "        res1 = C / (C + dists_x)\n",
    "        res1 += C / (C + dists_y)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            res1 = (1 - torch.eye(batch_size).to(device)) * res1\n",
    "        else:\n",
    "            res1 = (1 - torch.eye(batch_size)) * res1\n",
    "\n",
    "        res1 = res1.sum() / (batch_size - 1)\n",
    "        res2 = C / (C + dists_c)\n",
    "        res2 = res2.sum() * 2. / (batch_size)\n",
    "        stats += res1 - res2\n",
    "\n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "same_seeds(1) \n",
    "\n",
    "params = {\n",
    "    'num_epoch' : 350,\n",
    "    'batch_size' : 60,\n",
    "    'lr' : 4.8e-4,\n",
    "    'weight_decay' : 0.00000,\n",
    "    'sigma' : 5.0,\n",
    "    'MMD_lambda' : 2e-4,\n",
    "    'model_name' : 'FWAE',\n",
    "} \n",
    "all = pd.read_excel('data.xlsx', header=0).iloc[:,0:25].to_numpy()\n",
    "print(all.shape)\n",
    "raw_x = all[:,0:7]\n",
    "raw_descriptors = all[:,7:24]\n",
    "raw_y = all[:, 24].reshape(-1,1)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "raw_descriptors = scaler.fit_transform(raw_descriptors)\n",
    "\n",
    "\n",
    "dataset = FeatureDataset(raw_x[:], raw_y[:], raw_descriptors[:]) \n",
    "dataloader = DataLoader(dataset, batch_size=params['batch_size'], shuffle=True) \n",
    "print(raw_x[0:10],raw_y[0:10],raw_descriptors[0:5])\n",
    "print(raw_x.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx  \n",
    "from matplotlib.patches import Patch\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "raw_descriptors_df = pd.DataFrame(raw_descriptors)\n",
    "\n",
    "\n",
    "correlation_matrix = raw_descriptors_df.corr(method='pearson')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', vmin=-1, vmax=1, square=True)\n",
    "plt.title('Correlation Matrix of Raw Condition', fontsize=16, fontweight='bold', color='black')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "correlation_matrix.to_excel('Pearson_correlation_coefficient.xlsx', index=True)\n",
    "print(\"皮尔逊相关系数矩阵已保存为 Pearson_correlation_coefficient.xlsx\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "threshold = 0.9\n",
    "\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "\n",
    "features = correlation_matrix.columns.astype(str)\n",
    "G.add_nodes_from(features)\n",
    "\n",
    "\n",
    "for i in range(len(features)):\n",
    "    for j in range(i+1, len(features)):\n",
    "        feature_i = features[i]\n",
    "        feature_j = features[j]\n",
    "        corr_value = correlation_matrix.iloc[i, j]\n",
    "        if abs(corr_value) >= threshold:\n",
    "            G.add_edge(feature_i, feature_j)\n",
    "\n",
    "\n",
    "connected_components = list(nx.connected_components(G))\n",
    "\n",
    "\n",
    "feature_groups = [list(component) for component in connected_components if len(component) > 1]\n",
    "\n",
    "\n",
    "grouped_features = set([feature for group in feature_groups for feature in group])\n",
    "single_features = [feature for feature in features if feature not in grouped_features]\n",
    "\n",
    "\n",
    "print(\"\\n特征分组（相关性 >= 0.9）：\")\n",
    "for idx, group in enumerate(feature_groups, 1):\n",
    "    \n",
    "    group_str = ', '.join([str(feature) for feature in group])\n",
    "    print(f\"组 {idx}: {group_str}\")\n",
    "\n",
    "\n",
    "if single_features:\n",
    "    single_features_str = ', '.join([str(feature) for feature in single_features])\n",
    "    print(\"\\n单独的特征（没有与其他特征相关性 >= 0.9）：\")\n",
    "    print(single_features_str)\n",
    "\n",
    "\n",
    "group_data = {\n",
    "    'Group': [],\n",
    "    'Features': []\n",
    "}\n",
    "\n",
    "for idx, group in enumerate(feature_groups, 1):\n",
    "    group_data['Group'].append(f'Group_{idx}')\n",
    "    group_data['Features'].append(', '.join([str(feature) for feature in group]))\n",
    "\n",
    "\n",
    "for idx, feature in enumerate(single_features, len(feature_groups) + 1):\n",
    "    group_data['Group'].append(f'Group_{idx}')\n",
    "    group_data['Features'].append(str(feature))\n",
    "\n",
    "group_df = pd.DataFrame(group_data)\n",
    "\n",
    "\n",
    "group_df.to_excel('Feature_Groups.xlsx', index=False)\n",
    "print(\"\\n特征分组已保存为 Feature_Groups.xlsx\")\n",
    "\n",
    "\n",
    "feature_to_group = {}\n",
    "for idx, group in enumerate(feature_groups, 1):\n",
    "    for feature in group:\n",
    "        feature_to_group[feature] = f'Group_{idx}'\n",
    "for idx, feature in enumerate(single_features, len(feature_groups) + 1):\n",
    "    feature_to_group[feature] = f'Group_{idx}'\n",
    "\n",
    "\n",
    "num_groups = len(group_data['Group'])\n",
    "palette = sns.color_palette('tab20', num_groups)\n",
    "group_colors = {group: palette[idx] for idx, group in enumerate(group_data['Group'])}\n",
    "\n",
    "\n",
    "node_colors = [group_colors[feature_to_group[feature]] for feature in G.nodes()]\n",
    "\n",
    "\n",
    "layer_spacing = 3  \n",
    "group_centers = {}\n",
    "for idx, group in enumerate(group_data['Group'], 1):\n",
    "    group_centers[group] = (0, -layer_spacing * idx)  \n",
    "\n",
    "\n",
    "node_positions = {}\n",
    "max_group_size = max(group_df['Features'].apply(lambda x: len(x.split(', '))))  \n",
    "\n",
    "for group in group_data['Group']:\n",
    "    center_x, center_y = group_centers[group]\n",
    "    members = group_df[group_df['Group'] == group]['Features'].values[0].split(', ')\n",
    "    num_members = len(members)\n",
    "    if num_members == 1:\n",
    "        \n",
    "        node_positions[members[0]] = (center_x, center_y)\n",
    "    else:\n",
    "        \n",
    "        total_width = 8  \n",
    "        spacing = total_width / (num_members + 1)\n",
    "        for i, member in enumerate(members, 1):\n",
    "            x = center_x - total_width / 2 + spacing * i\n",
    "            y = center_y\n",
    "            node_positions[member] = (x, y)\n",
    "\n",
    "\n",
    "pos = node_positions  \n",
    "\n",
    "\n",
    "legend_elements = [Patch(facecolor=group_colors[f'Group_{i+1}'], edgecolor='black', label=f'Group_{i+1}') \n",
    "                   for i in range(num_groups)]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(14, 9))\n",
    "nx.draw_networkx_nodes(G, pos, node_size=500, node_color=node_colors, alpha=1)\n",
    "nx.draw_networkx_edges(G, pos, alpha=0.8)\n",
    "nx.draw_networkx_labels(G, pos, font_size=18, font_family='sans-serif')\n",
    "\n",
    "\n",
    "legend = plt.legend(handles=legend_elements, title='Feature Groups', \n",
    "                    bbox_to_anchor=(1.05, 1), loc='upper left', frameon=False)\n",
    "\n",
    "\n",
    "plt.setp(legend.get_title(), fontsize=22, fontweight='bold', color='black')\n",
    "for text in legend.get_texts():\n",
    "    text.set_fontsize(18)\n",
    "    text.set_fontweight('bold')\n",
    "    text.set_color('black')\n",
    "\n",
    "\n",
    "plt.axis('off')\n",
    "\n",
    "\n",
    "fig = plt.gcf()\n",
    "\n",
    "\n",
    "fig.patch.set_edgecolor('black')      \n",
    "fig.patch.set_linewidth(5)            \n",
    "fig.patch.set_facecolor('white')     \n",
    "\n",
    "\n",
    "plt.tight_layout(pad=3)\n",
    "\n",
    "\n",
    "plt.savefig('Feature_Groups_Visualization_Hierarchical.tiff', \n",
    "            format='tiff', dpi=600, \n",
    "            facecolor=fig.get_facecolor(), \n",
    "            edgecolor=fig.get_edgecolor(),\n",
    "            bbox_inches='tight')\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"特征分组可视化已保存为 Feature_Groups_Visualization_Hierarchical.tiff\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import itertools\n",
    "\n",
    "\n",
    "feature_groups = {\n",
    "    \"Group 1\": [5, 9, 15, 10, 4, 6, 7, 1, 2, 3, 0],\n",
    "    \"Group 2\": [13, 8],\n",
    "    \"Group 3\": [11, 14],\n",
    "    \"Group 4\": [12],\n",
    "    \"Group 5\": [16]\n",
    "}\n",
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "X = all[:, 7:24]\n",
    "y = all[:, 24].reshape(-1, 1)\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1)  \n",
    "\n",
    "\n",
    "def calculate_cv_rmse(X, y, selected_features):\n",
    "    if len(selected_features) == 0:\n",
    "        \n",
    "        return np.inf\n",
    "    temp_X = X[:, selected_features]\n",
    "    rmses = []\n",
    "    for train_index, test_index in kf.split(temp_X):\n",
    "        X_train, X_test = temp_X[train_index], temp_X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        \n",
    "        model = MLPRegressor(hidden_layer_sizes=(32, 16), max_iter=500, random_state=1)\n",
    "        model.fit(X_train, y_train.ravel())\n",
    "        y_pred = model.predict(X_test)\n",
    "        rmses.append(np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "    \n",
    "    return np.mean(rmses)\n",
    "\n",
    "\n",
    "def generate_valid_combinations(feature_groups, max_features):\n",
    "    group_names = list(feature_groups.keys())\n",
    "    group_features = [feature_groups[group] for group in group_names]\n",
    "\n",
    "    valid_combinations = []\n",
    "    for num_features in range(0, max_features + 1):  \n",
    "        for feature_combination in combinations(range(len(group_names)), num_features):\n",
    "            if feature_combination:  \n",
    "                \n",
    "                feature_lists = [group_features[group_idx] for group_idx in feature_combination]\n",
    "                for selected_features in itertools.product(*feature_lists):\n",
    "                    valid_combinations.append(selected_features)\n",
    "            else:\n",
    "                \n",
    "                valid_combinations.append(())\n",
    "    return valid_combinations\n",
    "\n",
    "\n",
    "valid_combinations = generate_valid_combinations(feature_groups, max_features=5)\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "for combination in valid_combinations:\n",
    "    combination_indices = list(combination)  \n",
    "    rmse = calculate_cv_rmse(X, y, combination_indices)\n",
    "    results.append({\n",
    "        \"Combination\": combination,\n",
    "        \"RMSE\": rmse\n",
    "    })\n",
    "    print(f\"Combination: {combination}, RMSE: {rmse}\")\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "output_file = \"Feature_Combinations_RMSE.xlsx\"\n",
    "results_df.to_excel(output_file, index=False)\n",
    "print(f\"Results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "same_seeds(1) \n",
    "params = {\n",
    "    'num_epoch' : 500,\n",
    "    'batch_size' : 80,\n",
    "    'lr' : 8e-4,\n",
    "    'weight_decay' : 0,\n",
    "    'sigma' : 3,\n",
    "    'MMD_lambda' : 1e-4,\n",
    "    'model_name' : 'FWAE',\n",
    "} \n",
    "\n",
    "all = pd.read_excel('data-1.xlsx', header=0).iloc[:,0:12].to_numpy()\n",
    "print(all.shape)\n",
    "raw_x = all[:,0:7]\n",
    "raw_descriptors = all[:,7:11]\n",
    "raw_y = all[:,11].reshape(-1,1)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "raw_descriptors = scaler.fit_transform(raw_descriptors)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset = FeatureDataset(raw_x[:], raw_y[:], raw_descriptors[:]) \n",
    "dataloader = DataLoader(dataset, batch_size=params['batch_size'], shuffle=True) \n",
    "print(raw_x[0:10],raw_y[0:10],raw_descriptors[0:5])\n",
    "print(raw_x.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "from torch.optim import AdamW\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def optimize_FWAE(lr, batch_size, MMD_lambda, sigma, weight_decay):\n",
    "    \n",
    "    params = {\n",
    "        'num_epoch': 500,\n",
    "        'batch_size': int(batch_size),  \n",
    "        'lr': lr,\n",
    "        'weight_decay': weight_decay,  \n",
    "        'sigma': sigma,\n",
    "        'MMD_lambda': MMD_lambda,\n",
    "        'model_name': 'FWAE',\n",
    "    }\n",
    "\n",
    "   \n",
    "    model = FWAE(raw_x.shape[1]).to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n",
    "\n",
    "    \n",
    "    loss_ = train_FWAE(model, optimizer, dataloader, params)\n",
    "\n",
    "    \n",
    "    return -min(loss_)  \n",
    "\n",
    "\n",
    "pbounds = {\n",
    "    'lr': (1e-6, 1e-3),           \n",
    "    'batch_size': (10, 70),      \n",
    "    'MMD_lambda': (1e-4, 1e-2),   \n",
    "    'sigma': (0.1, 5),            \n",
    "    'weight_decay': (1e-6, 1e-3)  \n",
    "}\n",
    "\n",
    "\n",
    "optimizer_bo = BayesianOptimization(\n",
    "    f=optimize_FWAE,  \n",
    "    pbounds=pbounds,  \n",
    "    random_state=1   \n",
    ")\n",
    "\n",
    "\n",
    "output_folder = os.path.join(os.getcwd(), 'reconstructeddata')\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "\n",
    "param_results = []\n",
    "\n",
    "\n",
    "def train_FWAE(model, optimizer, dataloader, params):\n",
    "    num_epoch = params['num_epoch']\n",
    "    sigma = params['sigma']  \n",
    "    MMD_lambda = params['MMD_lambda'] \n",
    "\n",
    "    loss_ = []  \n",
    "    for epoch in range(num_epoch):\n",
    "        total_loss = []  \n",
    "        for i, data in enumerate(dataloader):\n",
    "            x = data[0].to(device)  \n",
    "            y = data[1].to(device)  \n",
    "\n",
    "            model.train()  \n",
    "            recon_x, z_tilde = model(x)  \n",
    "\n",
    "            \n",
    "            z = sigma * torch.randn(z_tilde.size()).to(device)\n",
    "\n",
    "            \n",
    "            recon_loss = F.binary_cross_entropy(recon_x, x, reduction='mean')\n",
    "\n",
    "            \n",
    "            MMD_loss = imq_kernel(z_tilde, z, h_dim=2).to(device)\n",
    "            MMD_loss = MMD_loss / x.size(0)  \n",
    "            loss = recon_loss + MMD_loss * MMD_lambda  \n",
    "\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            \n",
    "            total_loss.append(loss.item())\n",
    "\n",
    "        avg_loss = sum(total_loss) / len(total_loss)\n",
    "        loss_.append(avg_loss)\n",
    "\n",
    "    return loss_\n",
    "\n",
    "\n",
    "optimizer_bo.maximize(init_points=10, n_iter=30)\n",
    "\n",
    "\n",
    "for res in optimizer_bo.res:\n",
    "    param_results.append({\n",
    "        'lr': res['params']['lr'],\n",
    "        'batch_size': res['params']['batch_size'],\n",
    "        'MMD_lambda': res['params']['MMD_lambda'],\n",
    "        'sigma': res['params']['sigma'],\n",
    "        'weight_decay': res['params']['weight_decay'],  \n",
    "        'loss': -res['target']  \n",
    "    })\n",
    "\n",
    "\n",
    "param_df = pd.DataFrame(param_results)\n",
    "output_file = os.path.join(output_folder, 'Reconstrction_parameters.xlsx')\n",
    "param_df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f'超参数组合和损失已保存到 {output_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "same_seeds(1) \n",
    "params = {\n",
    "    'num_epoch' : 500,\n",
    "    'batch_size' : 60,\n",
    "    'lr' : 0.0007521,\n",
    "    'weight_decay' : 0.0005393,\n",
    "    'sigma' : 1.25,\n",
    "    'MMD_lambda' : 0.0008325,\n",
    "    'model_name' : 'FWAE',\n",
    "} \n",
    "\n",
    "all = pd.read_excel('data-1.xlsx', header=0).iloc[:,0:12].to_numpy()\n",
    "print(all.shape)\n",
    "raw_x = all[:,0:7]\n",
    "raw_descriptors = all[:,7:11]\n",
    "raw_y = all[:,11].reshape(-1,1)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "raw_descriptors = scaler.fit_transform(raw_descriptors)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset = FeatureDataset(raw_x[:], raw_y[:], raw_descriptors[:]) \n",
    "dataloader = DataLoader(dataset, batch_size=params['batch_size'], shuffle=True) \n",
    "print(raw_x[20:40],raw_y[20:40],raw_descriptors[0:5])\n",
    "print(raw_x.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.optim import AdamW\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "model = FWAE(raw_x.shape[1]).to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n",
    "\n",
    "def train_FWAE(model, optimizer, dataloader, params):\n",
    "    model_name = params['model_name']\n",
    "    num_epoch = params['num_epoch']\n",
    "    sigma = params['sigma']\n",
    "    MMD_lambda = params['MMD_lambda']\n",
    "\n",
    "    folder_dir = os.path.join(os.getcwd(), model_name)\n",
    "    if not os.path.isdir(folder_dir):\n",
    "        os.mkdir(folder_dir)\n",
    "\n",
    "    loss_ = []\n",
    "    recon_ = []\n",
    "    MMD_ = []\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        total_loss = []\n",
    "        total_recon = []\n",
    "        total_MMD = []\n",
    "\n",
    "        for i, data in enumerate(dataloader):\n",
    "            x = data[0].to(device)\n",
    "            y = data[1].to(device)\n",
    "\n",
    "            model.train()\n",
    "            recon_x, z_tilde = model(x)\n",
    "\n",
    "            z = sigma * torch.randn(z_tilde.size()).to(device)\n",
    "\n",
    "            recon_loss = F.binary_cross_entropy(recon_x, x, reduction='mean')\n",
    "            \n",
    "\n",
    "            MMD_loss = imq_kernel(z_tilde, z, h_dim=2).to(device)\n",
    "            MMD_loss = MMD_loss / x.size(0)\n",
    "            loss = recon_loss + MMD_lambda * MMD_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss.append(loss.item())\n",
    "            total_recon.append(recon_loss.item())\n",
    "            total_MMD.append(MMD_loss.item())\n",
    "\n",
    "        avg_loss = sum(total_loss) / len(total_loss)\n",
    "        avg_recon = sum(total_recon) / len(total_recon)\n",
    "        avg_MMD = sum(total_MMD) / len(total_MMD)\n",
    "\n",
    "        loss_.append(avg_loss)\n",
    "        recon_.append(avg_recon)\n",
    "        MMD_.append(avg_MMD)\n",
    "\n",
    "        print('[{:03}/{:03}] loss: {:.7f} Recon_loss: {:.7f}, MMD_loss: {:.7f}'.format(\n",
    "            epoch + 1, num_epoch, avg_loss, avg_recon, avg_MMD))\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            save_model_dir = f\"{model_name}_{epoch + 1}.pth\"\n",
    "            torch.save(model.state_dict(), os.path.join(folder_dir, save_model_dir))\n",
    "\n",
    "    return loss_, recon_, MMD_\n",
    "\n",
    "\n",
    "loss_, recon_, MMD_ = train_FWAE(model, optimizer, dataloader, params)\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'loss': loss_,\n",
    "    'recon_loss': recon_,\n",
    "    'MMD_loss': MMD_\n",
    "})\n",
    "df.to_excel('Reconstrction_loss.xlsx', index=False)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.set_style('ticks')\n",
    "plt.plot(loss_, label='Total Loss', linewidth=2)\n",
    "plt.plot(recon_, label='Reconstruction Loss', linewidth=2)\n",
    "plt.plot(MMD_, label='MMD Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('FWAE Training Losses')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.set_style('whitegrid')\n",
    "plt.plot(loss_, color='darkblue', label='Total Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Total Loss')\n",
    "plt.title('Total Loss over Epochs')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = params['model_name']\n",
    "folder_dir = os.path.join(os.getcwd(), model_name)\n",
    "\n",
    "\n",
    "reconstructed_folder = os.path.join(os.getcwd(), 'reconstructeddata')\n",
    "if not os.path.exists(reconstructed_folder):\n",
    "    os.makedirs(reconstructed_folder)\n",
    "\n",
    "\n",
    "model_dir = os.path.join(folder_dir, '{}_{}.pth'.format(params['model_name'], params['num_epoch']))\n",
    "model = FWAE(raw_x.shape[1]).to(device)  \n",
    "model.load_state_dict(torch.load(model_dir))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_x = torch.FloatTensor(raw_x).to(device)\n",
    "    print(\"test_x shape:\", test_x.shape)\n",
    "    \n",
    "    \n",
    "    recon_x, z = model(test_x)\n",
    "    \n",
    "    \n",
    "    recon_x = recon_x.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "column_name = ['Ba','Ca', 'Sr','Ti','Zr','Sn','Hf']\n",
    "\n",
    "\n",
    "pd.DataFrame(recon_x.round(3), columns=column_name).iloc[0:155]  \n",
    "\n",
    "df_x = pd.DataFrame(recon_x.round(5), columns=column_name)  \n",
    "csv_file_path_x = os.path.join(reconstructed_folder, 'reconstructed_x_data.csv')  \n",
    "df_x.to_csv(csv_file_path_x, index=False)\n",
    "\n",
    "print(f\"Reconstructed x data saved in folder: {reconstructed_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('ticks')\n",
    "model = FWAE(raw_x.shape[1]).to(device)\n",
    "model.load_state_dict(torch.load(model_dir))\n",
    "\n",
    "dataset = FeatureDataset(raw_x[:], raw_y[:])\n",
    "latents = get_latents(model, dataset)\n",
    "print(latents.shape)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(figsize = (8, 5),dpi=300)\n",
    "\n",
    "\n",
    "axs.set_xlim(np.min(latents[:,0] - 0.5), np.max(latents[:,0] + 0.5))\n",
    "axs.set_ylim(np.min(latents[:,1] - 0.5), np.max(latents[:,1] + 0.5))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for axis in ['top','bottom','left','right']:\n",
    "  axs.spines[axis].set_linewidth(1.)\n",
    "\n",
    "axs.tick_params(axis='both', which='major', top=False, labeltop=False, direction='out', width=1., length=3)\n",
    "axs.tick_params(axis='both', which='major', right=False, labelright=False, direction='out', width=1, length=3)\n",
    "\n",
    "scatter1 = axs.scatter(latents[:, 0], latents[:, 1], c='turquoise', alpha=1, s=32, linewidths=0, label='BT-Based')\n",
    "\n",
    "axs.set_xlabel('Latent Dimension 1', fontsize=12)\n",
    "axs.set_ylabel('Latent Dimension 2', fontsize=12)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'latents_0': latents[:, 0],\n",
    "    'latents_1': latents[:, 1]\n",
    "})\n",
    "\n",
    "\n",
    "excel_filename = 'table_latent.xlsx'\n",
    "\n",
    "\n",
    "df.to_excel(excel_filename, index=False)\n",
    "\n",
    "handles,labels = axs.get_legend_handles_labels()\n",
    "handles = handles[::1]\n",
    "labels = labels[::1]\n",
    "\n",
    "legend_properties = {'size':12}\n",
    "axs.legend(handles, labels, loc='upper left', bbox_to_anchor=(1.015,0.275), handletextpad=0, frameon=False, prop=legend_properties)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "folder_name = 'n_components'\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "n_components_range = range(1, 11) \n",
    "aic_scores = []\n",
    "bic_scores = []\n",
    "log_likelihoods = []\n",
    "\n",
    "for n_components in n_components_range:\n",
    "    \n",
    "    gm = GaussianMixture(n_components=n_components, random_state=1, init_params='kmeans').fit(latents)\n",
    "    \n",
    "    \n",
    "    aic_scores.append(gm.aic(latents))\n",
    "    bic_scores.append(gm.bic(latents))\n",
    "    \n",
    "    \n",
    "    log_likelihood = gm.score(latents) * len(latents)  \n",
    "    log_likelihoods.append(log_likelihood)\n",
    "\n",
    "\n",
    "aic_bic_df = pd.DataFrame({\n",
    "    'n_components': n_components_range,\n",
    "    'AIC': aic_scores,\n",
    "    'BIC': bic_scores\n",
    "})\n",
    "\n",
    "\n",
    "log_likelihoods_df = pd.DataFrame({\n",
    "    'n_components': n_components_range,\n",
    "    'Log Likelihood': log_likelihoods\n",
    "})\n",
    "\n",
    "\n",
    "aic_bic_file_path = os.path.join(folder_name, 'AIC_BIC.xlsx')\n",
    "aic_bic_df.to_excel(aic_bic_file_path, index=False)\n",
    "\n",
    "\n",
    "log_likelihoods_file_path = os.path.join(folder_name, 'log_likelihoods.xlsx')\n",
    "log_likelihoods_df.to_excel(log_likelihoods_file_path, index=False)\n",
    "\n",
    "\n",
    "print(f\"AIC/BIC 保存到 {aic_bic_file_path}\")\n",
    "print(f\"Log Likelihoods 保存到 {log_likelihoods_file_path}\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(n_components_range, aic_scores, label='AIC', marker='o')\n",
    "plt.plot(n_components_range, bic_scores, label='BIC', marker='o')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('AIC / BIC Score')\n",
    "plt.legend()\n",
    "aic_bic_plot_path = os.path.join(folder_name, 'AIC_BIC.png')\n",
    "plt.savefig(aic_bic_plot_path)  \n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(n_components_range, log_likelihoods, label='Log Likelihood', marker='o', color='green')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Log Likelihood')\n",
    "plt.legend()\n",
    "log_likelihoods_plot_path = os.path.join(folder_name, 'log_likelihoods.png')\n",
    "plt.savefig(log_likelihoods_plot_path)  \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(f\"AIC_BIC 图像保存到 {aic_bic_plot_path}\")\n",
    "print(f\"Log Likelihoods 图像保存到 {log_likelihoods_plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "from joblib import dump\n",
    "import matplotlib\n",
    "\n",
    "\n",
    "matplotlib.rcParams['font.family'] = 'Arial'\n",
    "\n",
    "def draw_ellipse(position, covariance, ax=None, scale_factor=1, **kwargs):\n",
    "    \"\"\"Draw an ellipse with a given position and covariance\"\"\"\n",
    "    ax = ax or plt.gca()\n",
    "    \n",
    "    if covariance.shape == (2, 2):\n",
    "        U, s, Vt = np.linalg.svd(covariance)\n",
    "        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))\n",
    "        width, height = 2 * np.sqrt(s) * scale_factor\n",
    "    else:\n",
    "        angle = 0\n",
    "        width, height = 2 * np.sqrt(covariance) * scale_factor\n",
    "\n",
    "    for nsig in range(1, 4):\n",
    "        ellipse = Ellipse(\n",
    "            xy=position,\n",
    "            width=nsig * width,\n",
    "            height=nsig * height,\n",
    "            angle=angle,\n",
    "            **kwargs\n",
    "        )\n",
    "        ax.add_patch(ellipse)\n",
    "\n",
    "\n",
    "color = 'turquoise'\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 7))\n",
    "\n",
    "n_components = 4\n",
    "gm = GaussianMixture(n_components=n_components, random_state=1, init_params='kmeans').fit(latents)\n",
    "print('Average negative log likelihood:', -1 * gm.score(latents))\n",
    "\n",
    "\n",
    "ax.scatter(latents[:, 0], latents[:, 1], s=30, marker='o', color=color, alpha=0.3)\n",
    "\n",
    "\n",
    "\n",
    "ax.set_xlabel('Dimension 1', fontsize=24, fontweight='bold', color='black')\n",
    "ax.set_ylabel('Dimension 2', fontsize=24, fontweight='bold', color='black')\n",
    "\n",
    "\n",
    "ax.tick_params(axis='both', labelsize=18, labelcolor='black')\n",
    "for tick_label in ax.get_xticklabels() + ax.get_yticklabels():\n",
    "    tick_label.set_fontweight('bold')\n",
    "    tick_label.set_color('black')\n",
    "\n",
    "\n",
    "x_min, x_max = np.min(latents[:, 0]) - 0.5, np.max(latents[:, 0]) + 0.5\n",
    "y_min, y_max = np.min(latents[:, 1]) - 0.5, np.max(latents[:, 1]) + 0.5\n",
    "max_range = max(x_max - x_min, y_max - y_min)\n",
    "x_center = (x_max + x_min) / 2\n",
    "y_center = (y_max + y_min) / 2\n",
    "\n",
    "ax.set_xlim(x_center - max_range / 2, x_center + max_range / 2)\n",
    "ax.set_ylim(y_center - max_range / 2, y_center + max_range / 2)\n",
    "\n",
    "\n",
    "print(\"Data variance:\", np.var(latents, axis=0))\n",
    "for covar in gm.covariances_:\n",
    "    print(\"Covariance matrix:\", covar)\n",
    "\n",
    "\n",
    "for pos, covar, w in zip(gm.means_, gm.covariances_, gm.weights_):\n",
    "    draw_ellipse(pos, covar, alpha=w * 0.7, color=color, ax=ax)\n",
    "\n",
    "\n",
    "dump(gm, 'gm_single_class.joblib')\n",
    "print(\"GMM model saved as 'gm_single_class.joblib'\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.savefig('Single Class.tiff', dpi=600, format='tiff')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of latents: {latents.shape}\")\n",
    "\n",
    "\n",
    "gm = GaussianMixture(n_components=4, random_state=1, init_params='kmeans').fit(latents)\n",
    "\n",
    "\n",
    "log_likelihoods = gm.score_samples(latents)\n",
    "\n",
    "\n",
    "for i, log_likelihood in enumerate(log_likelihoods):\n",
    "    print(f\"Data point {i + 1}: Log Likelihood = {log_likelihood}\")\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Log Likelihood': log_likelihoods\n",
    "})\n",
    "\n",
    "df.to_csv(\"log_likelihoods.csv\", index_label=\"Data Point\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_random_seed(1)\n",
    "\n",
    "\n",
    "class AttributeDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.Tensor(self.x[idx]), torch.Tensor(self.y[idx])\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2, 256),  \n",
    "            nn.ReLU(),         \n",
    "            nn.Dropout(0.48),\n",
    "            nn.Linear(256, 128),  \n",
    "            nn.ReLU(),         \n",
    "            nn.Dropout(0.48),\n",
    "            nn.Linear(128, 32), \n",
    "            nn.ReLU(),         \n",
    "            nn.Dropout(0.48),\n",
    "            nn.Linear(32, 16),  \n",
    "            nn.ReLU(),         \n",
    "            nn.Dropout(0.48),\n",
    "            nn.Linear(16, 1),   \n",
    "            nn.Sigmoid()        \n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "params = {\n",
    "    'cls_bs': 15,  \n",
    "    'cls_lr': 8e-4,  \n",
    "    'cls_epoch': 500,  \n",
    "    'num_fold': 6,  \n",
    "    'latents': latents,  \n",
    "}\n",
    "\n",
    "\n",
    "threshold = 300\n",
    "label_y = np.where(raw_y < threshold, 1, 0)  \n",
    "params['label_y'] = label_y  \n",
    "\n",
    "\n",
    "folder_name = 'classification_results'\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "\n",
    "cls = Classifier().to(device)\n",
    "opt = Adam(cls.parameters(), lr=params['cls_lr'], weight_decay=0.00001)\n",
    "\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "scheduler = StepLR(opt, step_size=20, gamma=0.5)  \n",
    "\n",
    "\n",
    "def training_Cls(model, optimizer, scheduler, params):\n",
    "    results = {'train_loss': [], 'test_loss': [], 'train_accuracy': [], 'test_accuracy': []}\n",
    "    \n",
    "    label_y = params['label_y']\n",
    "    latents = params['latents']\n",
    "    cls_epoch = params['cls_epoch']\n",
    "    kf = KFold(n_splits=params['num_fold'], shuffle=True, random_state=1)\n",
    "    \n",
    "    fold = 0\n",
    "    for train, test in kf.split(latents, label_y):\n",
    "        x_train, x_test = latents[train], latents[test]\n",
    "        y_train, y_test = label_y[train], label_y[test]\n",
    "\n",
    "        \n",
    "        cls_dataset = AttributeDataset(x_train, y_train)\n",
    "        cls_dataloader = DataLoader(cls_dataset, batch_size=params['cls_bs'], shuffle=True)\n",
    "        cls_testDataset = AttributeDataset(x_test, y_test)\n",
    "        cls_testDataloader = DataLoader(cls_testDataset, batch_size=cls_testDataset.__len__(), shuffle=False)\n",
    "\n",
    "        for epoch in range(cls_epoch):\n",
    "            model.train()\n",
    "            total_train_loss, correct_train_preds = 0, 0\n",
    "            for i, data in enumerate(cls_dataloader):\n",
    "                x = data[0].to(device)\n",
    "                y = data[1].to(device).view(-1, 1)  \n",
    "                y_pred = model(x)\n",
    "                loss = F.binary_cross_entropy(y_pred, y.float())   \n",
    "\n",
    "                pred_labels = (y_pred > 0.5).float()  \n",
    "                correct_train_preds += (pred_labels == y).sum().item()\n",
    "                total_train_loss += loss.item()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            train_accuracy = correct_train_preds / len(x_train)\n",
    "\n",
    "            \n",
    "            model.eval()\n",
    "            total_test_loss, correct_test_preds = 0, 0\n",
    "            with torch.no_grad():\n",
    "                for test in cls_testDataloader:\n",
    "                    x = test[0].to(device)\n",
    "                    y = test[1].to(device).view(-1, 1)  \n",
    "                    y_pred = model(x)\n",
    "                    test_loss = F.binary_cross_entropy(y_pred, y.float())\n",
    "\n",
    "                    pred_labels = (y_pred > 0.5).float()\n",
    "                    correct_test_preds += (pred_labels == y).sum().item()\n",
    "                    total_test_loss += test_loss.item()\n",
    "\n",
    "            test_accuracy = correct_test_preds / len(x_test)\n",
    "\n",
    "            print(f'[{fold}/{params[\"num_fold\"]}] Epoch {epoch+1}/{cls_epoch} '\n",
    "                  f'Train Loss: {total_train_loss/len(cls_dataloader):.4f}, '\n",
    "                  f'Test Loss: {total_test_loss/len(cls_testDataloader):.4f}, '\n",
    "                  f'Train Accuracy: {train_accuracy:.4f}, '\n",
    "                  f'Test Accuracy: {test_accuracy:.4f}')\n",
    "        \n",
    "        results['train_loss'].append(total_train_loss / len(cls_dataloader))\n",
    "        results['test_loss'].append(total_test_loss / len(cls_testDataloader))\n",
    "        results['train_accuracy'].append(train_accuracy)\n",
    "        results['test_accuracy'].append(test_accuracy)\n",
    "\n",
    "        fold += 1\n",
    "\n",
    "        \n",
    "        scheduler.step()\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "results = training_Cls(cls, opt, scheduler, params)\n",
    "\n",
    "torch.save(cls.state_dict(), 'classifier_model.pth')\n",
    "print(\"模型已保存为 'classifier_model.pth'\")\n",
    "\n",
    "\n",
    "df_loss = pd.DataFrame({'Train Loss': results['train_loss'], 'Test Loss': results['test_loss']})\n",
    "df_accuracy = pd.DataFrame({'Train Accuracy': results['train_accuracy'], 'Test Accuracy': results['test_accuracy']})\n",
    "\n",
    "df_loss.to_excel(os.path.join(folder_name, 'Loss.xlsx'), index=False)\n",
    "df_accuracy.to_excel(os.path.join(folder_name, 'Accuracy.xlsx'), index=False)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(results['train_loss'], label='Train Loss')\n",
    "plt.plot(results['test_loss'], label='Test Loss')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(results['train_accuracy'], label='Train Accuracy')\n",
    "plt.plot(results['test_accuracy'], label='Test Accuracy')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import joblib  \n",
    "import torch.nn.functional as F\n",
    "\n",
    "np.random.seed(1)\n",
    "def MCMC(gms, cls, n_samples, sigma=0.1):\n",
    "    sample_z = []\n",
    "    all_z_trace = []  \n",
    "\n",
    "    \n",
    "    z = gms.sample(1)[0]  \n",
    "\n",
    "    for i in range(n_samples):\n",
    "        uniform_rand = np.random.uniform(size=1)\n",
    "        \n",
    "        z_next = np.random.multivariate_normal(z.squeeze(), sigma * np.eye(2)).reshape(1, -1)\n",
    "\n",
    "        z_combined = np.concatenate((z, z_next), axis=0)  \n",
    "\n",
    "        \n",
    "        outputs = cls(torch.Tensor(z_combined).to(device)).detach().cpu().numpy().squeeze()\n",
    "        z_output, z_next_output = outputs[0], outputs[1]  \n",
    "\n",
    "        \n",
    "        z_prob = gms.score(z) + np.log(max(z_output, 1e-8))  \n",
    "        z_next_prob = gms.score(z_next) + np.log(max(z_next_output, 1e-8))\n",
    "\n",
    "        \n",
    "        acceptance = min(0, (z_next_prob - z_prob))\n",
    "\n",
    "        \n",
    "        if i == 0:\n",
    "            sample_z.append(z.squeeze())\n",
    "            \n",
    "        \n",
    "        all_z_trace.append(z_next.squeeze())\n",
    "\n",
    "        if np.log(uniform_rand) < acceptance:\n",
    "            sample_z.append(z_next.squeeze())\n",
    "            z = z_next  \n",
    "\n",
    "    return np.stack(sample_z), np.stack(all_z_trace)\n",
    "\n",
    "\n",
    "cls = Classifier().to(device)  \n",
    "cls.load_state_dict(torch.load('classifier_model.pth'))  \n",
    "cls.eval()  \n",
    "\n",
    "gms = joblib.load('gm_single_class.joblib')  \n",
    "\n",
    "\n",
    "n_samples = 30000\n",
    "sample_z_array, all_z_trace_array = MCMC(gms=gms, cls=cls, n_samples=30000, sigma=0.5)\n",
    "\n",
    "sample_z_tensor = torch.tensor(sample_z_array, dtype=torch.float32).to(device)\n",
    "all_z_trace_tensor = torch.tensor(all_z_trace_array, dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "latent_df = pd.DataFrame(sample_z_array, columns=['latent_0', 'latent_1'])\n",
    "latent_df.to_csv('latent_samples_before_decode.csv', index=False)\n",
    "\n",
    "\n",
    "trace_df = pd.DataFrame(all_z_trace_array, columns=['latent_0', 'latent_1'])\n",
    "trace_df.to_csv('latent_trace_all_points.csv', index=False)\n",
    "\n",
    "\n",
    "WAE_comps = model._decode(sample_z_tensor.float().to(device)).detach().cpu().numpy()\n",
    "\n",
    "\n",
    "WAE_comps = pd.DataFrame(WAE_comps)\n",
    "WAE_comps.columns = column_name  \n",
    "WAE_comps.to_csv('comps_samples.csv', index=False, encoding='utf-8')\n",
    "\n",
    "\n",
    "print(WAE_comps.shape[0])\n",
    "\n",
    "tolerance = 1e-3\n",
    "\n",
    "\n",
    "filtered_latents = []\n",
    "filtered_decoded = []\n",
    "\n",
    "\n",
    "for z_vec, decoded_vec in zip(sample_z_tensor.cpu().numpy(), WAE_comps.values):\n",
    "    decoded_vec = np.array(decoded_vec)  \n",
    "\n",
    "    sum_first3 = decoded_vec[:3].sum()\n",
    "    sum_last4 = decoded_vec[-4:].sum()\n",
    "\n",
    "    if abs(sum_first3 - 0.5) <= tolerance and abs(sum_last4 - 0.5) <= tolerance:\n",
    "        filtered_latents.append(z_vec)\n",
    "        filtered_decoded.append(decoded_vec)\n",
    "\n",
    "\n",
    "filtered_latents_df = pd.DataFrame(filtered_latents, columns=['latent_0', 'latent_1'])\n",
    "filtered_decoded_df = pd.DataFrame(filtered_decoded, columns=column_name)\n",
    "\n",
    "\n",
    "filtered_latents_df.to_csv('filtered_latents.csv', index=False, encoding='utf-8')\n",
    "filtered_decoded_df.to_csv('filtered_decoded_comps.csv', index=False, encoding='utf-8')\n",
    "\n",
    "\n",
    "print(\"筛选后样本数量：\", len(filtered_latents))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
