{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44299295-da24-42a7-8b7e-e08b4cecf45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib  \n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "class MAPELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MAPELoss, self).__init__()\n",
    "    \n",
    "    def forward(self, output, target):\n",
    "        loss = torch.mean(torch.abs((target - output) / (target)))\n",
    "        return loss\n",
    "\n",
    "class MAELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MAELoss, self).__init__()\n",
    "    \n",
    "    def forward(self, output, target):\n",
    "        loss = torch.mean(torch.abs(target - output))\n",
    "        return loss        \n",
    "\n",
    "class MSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MSELoss, self).__init__()\n",
    "    \n",
    "    def forward(self, output, target):\n",
    "        loss = torch.mean((target - output) ** 2)\n",
    "        return loss        \n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "class FeatureDataset(Dataset):\n",
    "    '''\n",
    "    Args: x is a 2D numpy array [x_size, x_features]\n",
    "    '''\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.FloatTensor(self.x[idx])\n",
    "\n",
    "    def getBatch(self, idxs=[]):\n",
    "        if idxs == None:\n",
    "            return idxs\n",
    "        else:\n",
    "            x_features = []\n",
    "            for i in idxs:\n",
    "                x_features.append(self.__getitem__(i))\n",
    "            return torch.FloatTensor(x_features)\n",
    "\n",
    "def normalizing_data(data, seed=1):  \n",
    "    \n",
    "    composition = data[['Ba', 'Ca', 'Sr', 'Ti', 'Zr','Sn', 'Hf']]\n",
    "    descriptors = data[['W', 'EI', 'EA', 'μ']]\n",
    "    \n",
    "    \n",
    "    composition_scaler = MinMaxScaler()\n",
    "    normalized_composition = composition_scaler.fit_transform(composition)\n",
    "    descriptors_scaler = MinMaxScaler()\n",
    "    normalized_descriptors = descriptors_scaler.fit_transform(descriptors)\n",
    "    \n",
    "    \n",
    "    joblib.dump(composition_scaler, 'composition_scaler.joblib')  \n",
    "    joblib.dump(descriptors_scaler, 'descriptors_scaler.joblib')  \n",
    "    \n",
    "    \n",
    "    normalized_composition_df = pd.DataFrame(normalized_composition, columns=composition.columns)\n",
    "    normalized_descriptors_df = pd.DataFrame(normalized_descriptors, columns=descriptors.columns)\n",
    "    \n",
    "    \n",
    "    x = pd.concat([normalized_composition_df, normalized_descriptors_df], axis=1)\n",
    "    print(x)\n",
    "    \n",
    "    \n",
    "    y = data[['d33(pC/N)']]  \n",
    "    print(y)\n",
    "\n",
    "    \n",
    "    x = torch.FloatTensor(x.values)\n",
    "    y = torch.FloatTensor(y.values)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "    \n",
    "    \n",
    "    train_features, test_features, train_labels, test_labels = train_test_split(x, y, test_size=0.2, random_state=seed)\n",
    "    print(y)\n",
    "    return x, y, train_features, test_features, train_labels, test_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3f6b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import torch.utils.data as Data\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F    \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn import preprocessing\n",
    "\n",
    "  \n",
    "table = pd.DataFrame(columns=['target','batch_size','lr','module__n_hidden','module__w'])\n",
    "\n",
    "plt.close('all')\n",
    "starttime = datetime.datetime.now()\n",
    "data = pd.read_excel('data-1.xlsx')\n",
    "\n",
    "\n",
    "x, y, train_features, test_features, train_labels, test_labels = normalizing_data(data,seed=1)\n",
    "print(train_features)\n",
    "print(train_labels)\n",
    "print(train_features.shape)  \n",
    "print(train_labels.shape)   \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb065ec-1635-4cb9-87b4-203aa0abbc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import torch.utils.data as Data\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F    \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn import preprocessing\n",
    "import os\n",
    "\n",
    "\n",
    "class Net(nn.Module):  \n",
    "    def __init__(self, n_feature = 11, n_hidden = 64, n_output = 1, w = 6):\n",
    "        super(Net, self).__init__()    \n",
    "        \n",
    "        self.hidden1 = torch.nn.Linear(n_feature, n_hidden) \n",
    "        nn.init.kaiming_normal_(self.hidden1.weight)\n",
    "        \n",
    "        self.hiddens = nn.ModuleList ([nn.Linear(n_hidden, n_hidden) for i in range(w)])                            \n",
    "        for m in self.hiddens:\n",
    "            nn.init.kaiming_normal_(m.weight)   \n",
    "        \n",
    "        self.predict = torch.nn.Linear(n_hidden, n_output)  \n",
    "        nn.init.kaiming_normal_(self.predict.weight)\n",
    "\n",
    "    def forward(self, x):  \n",
    "        x = self.hidden1(x)\n",
    "        \n",
    "        \n",
    "        x = F.relu(x)   \n",
    "        \n",
    "        for m in self.hiddens:\n",
    "            x = m(x)\n",
    "            \n",
    "            x = F.relu(x) \n",
    "                   \n",
    "        x = self.predict(x)\n",
    "        \n",
    "        return x\n",
    "       \n",
    "\n",
    "def train(net, num_epochs, batch_size, train_features, test_features, train_labels, test_labels,\n",
    "          train_loader,\n",
    "          optimizer):\n",
    "    print (\"\\n=== train begin ===\")\n",
    "    \n",
    "    train_ls, test_ls = [], []\n",
    "    loss = MAPELoss() \n",
    "    for epoch in range(num_epochs):\n",
    "        for x, y in train_loader:\n",
    "            ls = loss(net(x).view(-1, 1), y.view(-1, 1))\n",
    "            optimizer.zero_grad()\n",
    "            ls.backward()\n",
    "            optimizer.step()\n",
    "        if epoch % 100 == 0:\n",
    "            train_ls.append(loss(net(train_features).view(-1, 1), train_labels.view(-1, 1)).item())\n",
    "            test_ls.append(loss(net(test_features).view(-1, 1), test_labels.view(-1, 1)).item())\n",
    "            print (\"epoch %d: train loss %f, test loss %f\" % (epoch, train_ls[-1], test_ls[-1]))\n",
    "        \n",
    "    print (\"=== train end ===\")\n",
    "    \n",
    "def test(model, test_loader, set_name=\"Test set\"):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    n = 0\n",
    "    loss = MAPELoss() \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            test_loss += loss(output.view(-1, 1), target.view(-1, 1)).item()  \n",
    "            n += 1\n",
    "\n",
    "    test_loss /= n\n",
    "    \n",
    "    print(f'{set_name}: Average loss: {test_loss:.4f}')\n",
    "    return test_loss\n",
    "        \n",
    "def plotCurve(x_vals, y_vals, \n",
    "                     x_label, y_label, \n",
    "                     x2_vals=None, y2_vals=None, \n",
    "                    legend=None,\n",
    "                    figsize=(3.5, 2.5)):\n",
    "            \n",
    "            plt.xlabel(x_label)\n",
    "            plt.ylabel(y_label)\n",
    "            plt.plot(x_vals, y_vals)\n",
    "            if x2_vals and y2_vals:\n",
    "                plt.plot(x2_vals, y2_vals, linestyle=':')\n",
    "            \n",
    "            if legend:\n",
    "                plt.legend(legend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d128774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import torch.utils.data as Data\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F    \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from bayes_opt import BayesianOptimization\n",
    "import time\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "output_dir = 'NNBayesian'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "\n",
    "def train_model(batch_size, lr, module__n_hidden, module__w):\n",
    "    module__n_hidden = int(module__n_hidden)  \n",
    "    module__w = int(module__w)  \n",
    "    batch_size = int(batch_size)\n",
    "\n",
    "    train_dataset = Data.TensorDataset(train_features, train_labels)\n",
    "    test_dataset = Data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "    train_loader = Data.DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "    test_loader = Data.DataLoader(test_dataset, batch_size, shuffle=True)\n",
    "\n",
    "    net = Net(n_feature=11, n_hidden=module__n_hidden, n_output=1, w=module__w)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  \n",
    "    net.to(device)  \n",
    "\n",
    "    n_epochs = 1000\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=0.0001)\n",
    "\n",
    "    train(net, n_epochs, batch_size, train_features, test_features, train_labels, test_labels, train_loader, optimizer)\n",
    "    \n",
    "\n",
    "    train_loss = test(net, train_loader, set_name=\"Training set\")  \n",
    "    test_loss = test(net, test_loader, set_name=\"Test set\")  \n",
    "\n",
    "    r = -np.abs(train_loss - test_loss)\n",
    "    return -test_loss\n",
    "\n",
    "\n",
    "bounds = {'lr': (0.0001, 0.001), 'batch_size': (32, 64), 'module__n_hidden': (64, 256), 'module__w': (2, 10)}\n",
    "\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=train_model,\n",
    "    pbounds=bounds,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "optimizer.maximize(init_points=100, n_iter=150)  \n",
    "\n",
    "\n",
    "result_list = []  \n",
    "\n",
    "\n",
    "for res in optimizer.res:\n",
    "    result_list.append(pd.DataFrame({'target': [res['target']],\n",
    "                                     'batch_size': [res['params']['batch_size']],\n",
    "                                     'lr': [res['params']['lr']],\n",
    "                                     'module__n_hidden': [res['params']['module__n_hidden']],\n",
    "                                     'module__w': [res['params']['module__w']]}))\n",
    "\n",
    "\n",
    "table = pd.concat(result_list, ignore_index=True)\n",
    "\n",
    "\n",
    "table = pd.concat([table, pd.DataFrame({'target': [optimizer.max['target']],\n",
    "                                        'batch_size': [optimizer.max['params']['batch_size']],\n",
    "                                        'lr': [optimizer.max['params']['lr']],\n",
    "                                        'module__n_hidden': [optimizer.max['params']['module__n_hidden']],\n",
    "                                        'module__w': [optimizer.max['params']['module__w']]})], \n",
    "                                        ignore_index=True)\n",
    "\n",
    "\n",
    "model_name = 'd33_inference_NN_{}'.format(datetime.datetime.now().strftime('%Y%m%d_%H%M%S'))\n",
    "file_name = os.path.join(output_dir, '{}.xlsx'.format(model_name))\n",
    "\n",
    "\n",
    "endtime = datetime.datetime.now()\n",
    "Rtime = endtime - starttime\n",
    "print(Rtime)\n",
    "\n",
    "\n",
    "table.to_excel(file_name, index=False)  \n",
    "print(\"保存结果至: \", file_name)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6ec011",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'd33_inference_NN'\n",
    "file_name = '{}.xlsx'.format(model_name)\n",
    "endtime = datetime.datetime.now()\n",
    "Rtime = endtime - starttime\n",
    "print(Rtime)\n",
    "table.to_excel(file_name)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd3cc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import torch.utils.data as Data\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F   \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import os\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "set_random_seed(1)\n",
    "\n",
    "folder_dir = 'Results/STU_NN_BO(100+150)_1'\n",
    "if not os.path.exists(folder_dir):\n",
    "    os.makedirs(folder_dir)  \n",
    "\n",
    "folder_dir_figures = os.path.join(folder_dir, 'Figures')\n",
    "if not os.path.exists(folder_dir_figures):\n",
    "    os.makedirs(folder_dir_figures)  \n",
    "\n",
    "t = time.localtime() \n",
    "plt.close('all')\n",
    "target = pd.read_excel('d33_inference_NN.xlsx')\n",
    "starttime = datetime.datetime.now()\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(columns=['Iteration', 'Seed', 'target', 'R2_Score_test', 'Figure_Path_test', 'R2_Score_train', 'Figure_Path_train', 'R2_Score_all', 'Figure_Path_all', 'Final_Train_Loss', 'Final_Test_Loss'])\n",
    "\n",
    "\n",
    "x_all, y_all, train_features, test_features, train_labels, test_labels = normalizing_data(data, seed=1)\n",
    "\n",
    "\n",
    "train_df = pd.DataFrame(train_features.cpu().numpy())\n",
    "test_df = pd.DataFrame(test_features.cpu().numpy())\n",
    "\n",
    "\n",
    "train_file_path = 'train_features.xlsx'\n",
    "test_file_path = 'test_features.xlsx'\n",
    "train_df.to_excel(train_file_path, index=True)  \n",
    "test_df.to_excel(test_file_path, index=True)   \n",
    "\n",
    "\n",
    "for i in range(0, 251):  \n",
    "    for j in range(1, 2):  \n",
    "        set_random_seed(1)\n",
    "\n",
    "        \n",
    "        tg = target.at[i, 'target']\n",
    "        lr = target.at[i, 'lr'] \n",
    "        module__n_hidden = target.at[i, 'module__n_hidden']\n",
    "        module__w = target.at[i, 'module__w']\n",
    "        batch_size = target.at[i, 'batch_size']\n",
    "\n",
    "        module__n_hidden = int(module__n_hidden)\n",
    "        module__w = int(module__w)\n",
    "        batch_size = int(batch_size)\n",
    "\n",
    "        \n",
    "        train_dataset = Data.TensorDataset(train_features, train_labels)\n",
    "        test_dataset = Data.TensorDataset(test_features, test_labels) \n",
    "        train_loader = Data.DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "        test_loader = Data.DataLoader(test_dataset, batch_size, shuffle=True) \n",
    "        \n",
    "        \n",
    "        class Net(nn.Module):  \n",
    "            def __init__(self, n_feature=11, n_hidden=module__n_hidden, n_output=1, w=module__w):\n",
    "                super(Net, self).__init__()   \n",
    "                self.hidden1 = torch.nn.Linear(n_feature, n_hidden) \n",
    "                nn.init.kaiming_normal_(self.hidden1.weight)\n",
    "                \n",
    "                self.hiddens = nn.ModuleList([nn.Linear(n_hidden, n_hidden) for _ in range(w)])                            \n",
    "                for m in self.hiddens:\n",
    "                    nn.init.kaiming_normal_(m.weight)   \n",
    "                    \n",
    "                self.dropout = nn.Dropout(p=0.01)\n",
    "                self.predict = torch.nn.Linear(n_hidden, n_output) \n",
    "                nn.init.kaiming_normal_(self.predict.weight)\n",
    "\n",
    "            def forward(self, x): \n",
    "                x = self.hidden1(x)\n",
    "                x = F.relu(x)   \n",
    "                x = self.dropout(x)\n",
    "                for m in self.hiddens:\n",
    "                    x = m(x)\n",
    "                    x = F.relu(x) \n",
    "                    x = self.dropout(x)          \n",
    "                x = self.predict(x)\n",
    "                return x\n",
    "\n",
    "        \n",
    "        net = Net()\n",
    "        if torch.cuda.is_available():\n",
    "            net = net.cuda()    \n",
    "        \n",
    "        train_ls, test_ls = [], []\n",
    "        loss = MAPELoss() \n",
    "        n_epochs = 1000\n",
    "        optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=0.0001)\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            for x, y in train_loader:\n",
    "                ls = loss(net(x).view(-1, 1), y.view(-1, 1))\n",
    "                optimizer.zero_grad()\n",
    "                ls.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            train_ls.append(loss(net(train_features).view(-1, 1), train_labels.view(-1, 1)).item())\n",
    "            test_ls.append(loss(net(test_features).view(-1, 1), test_labels.view(-1, 1)).item())\n",
    "\n",
    "        \n",
    "        loss_data = pd.DataFrame({'Epoch': range(1, n_epochs + 1), 'Train Loss': train_ls, 'Test Loss': test_ls})\n",
    "        loss_file_path = f'Results/STU_NN_BO(100+150)_1/loss_data_{i}_seed_{j}.xlsx'\n",
    "        loss_data.to_excel(loss_file_path, index=False)\n",
    "\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.plot(range(1, n_epochs + 1), train_ls, label=\"Train Loss\", color='blue')\n",
    "        plt.plot(range(1, n_epochs + 1), test_ls, label=\"Test Loss\", color='orange')\n",
    "        plt.legend()\n",
    "        plt.text(60, 0.7, f'Target={tg:.4f}', fontdict={'size': 20, 'color':  'red'})\n",
    "        fig_name_1 = f'Results/STU_NN_BO(100+150)_1/Figures/{i}-seed_{j}_loss.png'\n",
    "        plt.savefig(fig_name_1, format='png', dpi=300)\n",
    "\n",
    "        \n",
    "        net.eval()\n",
    "        predict_test = net(test_features.cuda()).cpu().data.numpy()\n",
    "        predict_train = net(train_features.cuda()).cpu().data.numpy()\n",
    "        predict_all = net(x_all.cuda()).cpu().data.numpy()\n",
    "\n",
    "        \n",
    "        pd.DataFrame({'Predicted': predict_train.flatten(), 'Actual': train_labels.cpu().data.numpy().flatten()}).to_excel(f'Results/STU_NN_BO(100+150)_1/predictions_train_{i}_seed_{j}.xlsx', index=False)\n",
    "        pd.DataFrame({'Predicted': predict_test.flatten(), 'Actual': test_labels.cpu().data.numpy().flatten()}).to_excel(f'Results/STU_NN_BO(100+150)_1/predictions_test_{i}_seed_{j}.xlsx', index=False)\n",
    "        pd.DataFrame({'Predicted': predict_all.flatten(), 'Actual': y_all.cpu().data.numpy().flatten()}).to_excel(f'Results/STU_NN_BO(100+150)_1/predictions_all_{i}_seed_{j}.xlsx', index=False)\n",
    "\n",
    "         \n",
    "        fig_name_2_test = f'Results/STU_NN_BO(100+150)_1/Figures/{i}-seed_{j}_test.png'\n",
    "        plt.figure()\n",
    "        sns.regplot(x=predict_test, y=test_labels.cpu().data.numpy(), color='red')\n",
    "        plt.text(min(predict_test), max(test_labels.cpu().data.numpy()), f'R2={r2_score(test_labels.cpu().data.numpy(), predict_test):.4f}', color='red')\n",
    "        plt.savefig(fig_name_2_test, format='png', dpi=300)\n",
    "\n",
    "        \n",
    "        fig_name_2_train = f'Results/STU_NN_BO(100+150)_1/Figures/{i}-seed_{j}_train.png'\n",
    "        plt.figure()\n",
    "        sns.regplot(x=predict_train, y=train_labels.cpu().data.numpy(), color='blue')\n",
    "        plt.text(min(predict_train), max(train_labels.cpu().data.numpy()), f'R2={r2_score(train_labels.cpu().data.numpy(), predict_train):.4f}', color='blue')\n",
    "        plt.savefig(fig_name_2_train, format='png', dpi=300)\n",
    "\n",
    "        \n",
    "        fig_name_2_all = f'Results/STU_NN_BO(100+150)_1/Figures/{i}-seed_{j}_all.png'\n",
    "        plt.figure()\n",
    "        sns.regplot(x=predict_train, y=train_labels.cpu().data.numpy(), color='blue', label=\"Train\")\n",
    "        sns.regplot(x=predict_test, y=test_labels.cpu().data.numpy(), color='red', label=\"Test\")\n",
    "        plt.text(min(predict_all), max(y_all.cpu().data.numpy()), f'R2={r2_score(y_all.cpu().data.numpy(), predict_all):.4f}', color='green')\n",
    "        plt.legend()\n",
    "        plt.savefig(fig_name_2_all, format='png', dpi=300)\n",
    "\n",
    "        \n",
    "        current_r2_test = r2_score(test_labels.cpu().data.numpy(), predict_test)\n",
    "        current_r2_train = r2_score(train_labels.cpu().data.numpy(), predict_train)\n",
    "        current_r2_all = r2_score(y_all.cpu().data.numpy(), predict_all)\n",
    "\n",
    "        \n",
    "        final_train_loss = train_ls[-1]  \n",
    "        final_test_loss = test_ls[-1]  \n",
    "        \n",
    "        \n",
    "        results_df = pd.concat([results_df, pd.DataFrame([{\n",
    "            'Iteration': i,\n",
    "            'Seed': j,\n",
    "            'target': tg,\n",
    "            'R2_Score_test': current_r2_test,\n",
    "            'Figure_Path_test': fig_name_2_test,\n",
    "            'R2_Score_train': current_r2_train,\n",
    "            'Figure_Path_train': fig_name_2_train,\n",
    "            'R2_Score_all': current_r2_all,\n",
    "            'Figure_Path_all': fig_name_2_all,\n",
    "            'Final_Train_Loss': final_train_loss,  \n",
    "            'Final_Test_Loss': final_test_loss   \n",
    "        }])], ignore_index=True)\n",
    "        \n",
    "        net_name = f'Results/STU_NN_BO(100+150)_1/{i}-seed_{j}.pt'\n",
    "        torch.save(net.state_dict(), net_name)\n",
    "\n",
    "\n",
    "results_df.to_csv('Results/STU_NN_BO(100+150)_1/results_summary_NN.csv', index=False)\n",
    "\n",
    "\n",
    "endtime = datetime.datetime.now()\n",
    "Rtime = endtime - starttime\n",
    "print(Rtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7eef21",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(\"model architecture:\")\n",
    "print(model)\n",
    "print(\"num:{}\".format(params))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
