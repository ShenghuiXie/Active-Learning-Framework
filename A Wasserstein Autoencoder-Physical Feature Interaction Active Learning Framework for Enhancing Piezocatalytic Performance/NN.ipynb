{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb2a7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "from sklearn import preprocessing\n",
    "import os  \n",
    "\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "\n",
    "class MAPELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MAPELoss, self).__init__()\n",
    "    \n",
    "    def forward(self, output, target):\n",
    "        eps = 1e-8\n",
    "        loss = torch.mean(torch.abs((target - output) / (target + eps)))\n",
    "        return loss\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "class FeatureDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.FloatTensor(self.x[idx]), torch.FloatTensor(self.y[idx])\n",
    "\n",
    "def normalizing_data(data, seed=1, save_path='normalized_data_excel'):  \n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    comp_cols = ['Ba', 'Ca', 'Sr', 'Ti', 'Zr', 'Sn', 'Hf']  \n",
    "    desc_cols = ['W', 'EI', 'EA', 'Î¼']                      \n",
    "    all_feat_cols = comp_cols + desc_cols                   \n",
    "    label_col = ['d33(pC/N)']                               \n",
    "\n",
    "    composition = data[comp_cols]\n",
    "    descriptors = data[desc_cols]\n",
    "    \n",
    "    y = data[label_col].values  \n",
    "\n",
    "    \n",
    "    all_features = pd.concat([composition, descriptors], axis=1).values\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        all_features, y, test_size=0.2, random_state=seed\n",
    "    )\n",
    "\n",
    "    \n",
    "    composition_scaler = MinMaxScaler()\n",
    "    x_train_comp = x_train[:, :7]\n",
    "    x_test_comp = x_test[:, :7]\n",
    "    x_train_comp_norm = composition_scaler.fit_transform(x_train_comp)\n",
    "    x_test_comp_norm = composition_scaler.transform(x_test_comp)\n",
    "\n",
    "    \n",
    "    descriptors_scaler = MinMaxScaler()\n",
    "    x_train_desc = x_train[:, 7:]\n",
    "    x_test_desc = x_test[:, 7:]\n",
    "    x_train_desc_norm = descriptors_scaler.fit_transform(x_train_desc)\n",
    "    x_test_desc_norm = descriptors_scaler.transform(x_test_desc)\n",
    "\n",
    "    \n",
    "    joblib.dump(composition_scaler, f'{save_path}/composition_scaler.joblib')\n",
    "    joblib.dump(descriptors_scaler, f'{save_path}/descriptors_scaler.joblib')\n",
    "\n",
    "    \n",
    "    x_train_norm = np.hstack([x_train_comp_norm, x_train_desc_norm])\n",
    "    x_test_norm = np.hstack([x_test_comp_norm, x_test_desc_norm])\n",
    "    \n",
    "    x_comp_all = composition_scaler.transform(all_features[:, :7])\n",
    "    x_desc_all = descriptors_scaler.transform(all_features[:, 7:])\n",
    "    x_all_norm = np.hstack([x_comp_all, x_desc_all])\n",
    "\n",
    "    \n",
    "    train_df = pd.DataFrame(x_train_norm, columns=all_feat_cols)\n",
    "    train_df[label_col[0]] = y_train  \n",
    "    train_df.to_excel(f'{save_path}/train_set.xlsx', index=False, engine='openpyxl')\n",
    "\n",
    "    \n",
    "    test_df = pd.DataFrame(x_test_norm, columns=all_feat_cols)\n",
    "    test_df[label_col[0]] = y_test  \n",
    "    test_df.to_excel(f'{save_path}/test_set.xlsx', index=False, engine='openpyxl')\n",
    "\n",
    "    \n",
    "    all_df = pd.DataFrame(x_all_norm, columns=all_feat_cols)\n",
    "    all_df[label_col[0]] = y\n",
    "    all_df.to_excel(f'{save_path}/all_set.xlsx', index=False, engine='openpyxl')\n",
    "\n",
    "    \n",
    "    print(f\"\\nâœ… å½’ä¸€åŒ–æ•°æ®å·²ä¿å­˜ä¸ºExcelï¼Œè·¯å¾„ï¼š{save_path}\")\n",
    "    print(f\"ğŸ“Š è®­ç»ƒé›†ï¼ˆç‰¹å¾+æ ‡ç­¾ï¼‰ï¼š{train_df.shape} | æµ‹è¯•é›†ï¼š{test_df.shape} | å…¨é‡é›†ï¼š{all_df.shape}\")\n",
    "    print(\"ğŸ“‹ ExcelåŒ…å«åˆ—åï¼Œå¯ç›´æ¥ç”¨Excelæ‰“å¼€æŸ¥çœ‹/ç¼–è¾‘\")\n",
    "\n",
    "    \n",
    "    print(\"\\nå½’ä¸€åŒ–æ•°æ®å½¢çŠ¶éªŒè¯ï¼š\")\n",
    "    print(\"å…¨é‡ç‰¹å¾ï¼š\", x_all_norm.shape, \"| è®­ç»ƒç‰¹å¾ï¼š\", x_train_norm.shape, \"| æµ‹è¯•ç‰¹å¾ï¼š\", x_test_norm.shape)\n",
    "    print(\"æ ‡ç­¾å½¢çŠ¶ï¼š\", y.shape)\n",
    "\n",
    "    return (x_all_norm, y,          \n",
    "            x_train_norm, x_test_norm,  \n",
    "            y_train, y_test,            \n",
    "            composition_scaler, descriptors_scaler)  \n",
    "\n",
    "\n",
    "def load_normalized_data_excel(load_path='normalized_data_excel'):\n",
    "    \n",
    "    all_feat_cols = ['Ba', 'Ca', 'Sr', 'Ti', 'Zr', 'Sn', 'Hf', 'W', 'EI', 'EA', 'Î¼']\n",
    "    label_col = 'd33(pC/N)'\n",
    "\n",
    "   \n",
    "    train_df = pd.read_excel(f'{load_path}/train_set.xlsx', engine='openpyxl')\n",
    "    test_df = pd.read_excel(f'{load_path}/test_set.xlsx', engine='openpyxl')\n",
    "    all_df = pd.read_excel(f'{load_path}/all_set.xlsx', engine='openpyxl')\n",
    "\n",
    "    \n",
    "    x_train_norm = train_df[all_feat_cols].values\n",
    "    y_train = train_df[[label_col]].values\n",
    "    x_test_norm = test_df[all_feat_cols].values\n",
    "    y_test = test_df[[label_col]].values\n",
    "    x_all_norm = all_df[all_feat_cols].values\n",
    "    y_all = all_df[[label_col]].values\n",
    "\n",
    "    \n",
    "    composition_scaler = joblib.load(f'{load_path}/composition_scaler.joblib')\n",
    "    descriptors_scaler = joblib.load(f'{load_path}/descriptors_scaler.joblib')\n",
    "\n",
    "    \n",
    "    print(f\"\\nâœ… å·²ä»{load_path}åŠ è½½Excelæ ¼å¼å½’ä¸€åŒ–æ•°æ®\")\n",
    "    print(f\"ğŸ“Š åŠ è½½æ•°æ®å½¢çŠ¶ï¼šè®­ç»ƒç‰¹å¾{x_train_norm.shape} | æµ‹è¯•ç‰¹å¾{x_test_norm.shape} | å…¨é‡ç‰¹å¾{x_all_norm.shape}\")\n",
    "\n",
    "    return x_all_norm, y_all, x_train_norm, x_test_norm, y_train, y_test, composition_scaler, descriptors_scaler\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    plt.close('all')\n",
    "    \n",
    "    data = pd.read_excel('data-1.xlsx', engine='openpyxl')\n",
    "    print(\"åŸå§‹æ•°æ®å½¢çŠ¶ï¼š\", data.shape)\n",
    "    print(\"åŸå§‹æ•°æ®åˆ—åï¼š\", data.columns.tolist())\n",
    "\n",
    "    \n",
    "    x_all, y_all, x_train, x_test, y_train, y_test, _, _ = normalizing_data(data, seed=1)\n",
    "\n",
    "   \n",
    "    train_dataset = FeatureDataset(x_train, y_train)\n",
    "    test_dataset = FeatureDataset(x_test, y_test)\n",
    "\n",
    "    \n",
    "    BATCH_SIZE = 32\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    \n",
    "    print(\"\\nè®­ç»ƒé›†DataLoaderå•æ‰¹æ¬¡å½¢çŠ¶ï¼š\")\n",
    "    for batch_x, batch_y in train_dataloader:\n",
    "        print(\"ç‰¹å¾æ‰¹æ¬¡ï¼š\", batch_x.shape, \"| æ ‡ç­¾æ‰¹æ¬¡ï¼š\", batch_y.shape)\n",
    "        break\n",
    "    print(\"æµ‹è¯•é›†DataLoaderå•æ‰¹æ¬¡å½¢çŠ¶ï¼š\")\n",
    "    for batch_x, batch_y in test_dataloader:\n",
    "        print(\"ç‰¹å¾æ‰¹æ¬¡ï¼š\", batch_x.shape, \"| æ ‡ç­¾æ‰¹æ¬¡ï¼š\", batch_y.shape)\n",
    "        break\n",
    "\n",
    "\n",
    "    print(\"\\nè®­ç»ƒç‰¹å¾å‰5è¡Œï¼š\\n\", x_train[:5])\n",
    "    print(\"è®­ç»ƒæ ‡ç­¾å‰5è¡Œï¼š\\n\", y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bb065ec-1635-4cb9-87b4-203aa0abbc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import torch.utils.data as Data\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F    \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn import preprocessing\n",
    "import os\n",
    "\n",
    "\n",
    "class Net(nn.Module):  \n",
    "    def __init__(self, n_feature = 11, n_hidden = 64, n_output = 1, w = 6):\n",
    "        super(Net, self).__init__()    \n",
    "        \n",
    "        self.hidden1 = torch.nn.Linear(n_feature, n_hidden) \n",
    "        nn.init.kaiming_normal_(self.hidden1.weight)\n",
    "        \n",
    "        self.hiddens = nn.ModuleList ([nn.Linear(n_hidden, n_hidden) for i in range(w)])                            \n",
    "        for m in self.hiddens:\n",
    "            nn.init.kaiming_normal_(m.weight)   \n",
    "        \n",
    "        self.predict = torch.nn.Linear(n_hidden, n_output)  \n",
    "        nn.init.kaiming_normal_(self.predict.weight)\n",
    "\n",
    "    def forward(self, x):  \n",
    "        x = self.hidden1(x)\n",
    "        \n",
    "        \n",
    "        x = F.relu(x)   \n",
    "        \n",
    "        for m in self.hiddens:\n",
    "            x = m(x)\n",
    "            \n",
    "            x = F.relu(x) \n",
    "                   \n",
    "        x = self.predict(x)\n",
    "        \n",
    "        return x\n",
    "       \n",
    "\n",
    "def train(net, num_epochs, batch_size, train_features, test_features, train_labels, test_labels,\n",
    "          train_loader,\n",
    "          optimizer):\n",
    "    print (\"\\n=== train begin ===\")\n",
    "    \n",
    "    train_ls, test_ls = [], []\n",
    "    loss = MAPELoss() \n",
    "    for epoch in range(num_epochs):\n",
    "        for x, y in train_loader:\n",
    "            ls = loss(net(x).view(-1, 1), y.view(-1, 1))\n",
    "            optimizer.zero_grad()\n",
    "            ls.backward()\n",
    "            optimizer.step()\n",
    "        if epoch % 100 == 0:\n",
    "            train_ls.append(loss(net(train_features).view(-1, 1), train_labels.view(-1, 1)).item())\n",
    "            test_ls.append(loss(net(test_features).view(-1, 1), test_labels.view(-1, 1)).item())\n",
    "            print (\"epoch %d: train loss %f, test loss %f\" % (epoch, train_ls[-1], test_ls[-1]))\n",
    "        \n",
    "    print (\"=== train end ===\")\n",
    "    \n",
    "def test(model, test_loader, set_name=\"Test set\"):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    n = 0\n",
    "    loss = MAPELoss() \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            test_loss += loss(output.view(-1, 1), target.view(-1, 1)).item()  \n",
    "            n += 1\n",
    "\n",
    "    test_loss /= n\n",
    "    \n",
    "    print(f'{set_name}: Average loss: {test_loss:.4f}')\n",
    "    return test_loss\n",
    "        \n",
    "def plotCurve(x_vals, y_vals, \n",
    "                     x_label, y_label, \n",
    "                     x2_vals=None, y2_vals=None, \n",
    "                    legend=None,\n",
    "                    figsize=(3.5, 2.5)):\n",
    "            \n",
    "            plt.xlabel(x_label)\n",
    "            plt.ylabel(y_label)\n",
    "            plt.plot(x_vals, y_vals)\n",
    "            if x2_vals and y2_vals:\n",
    "                plt.plot(x2_vals, y2_vals, linestyle=':')\n",
    "            \n",
    "            if legend:\n",
    "                plt.legend(legend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc387193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import torch.utils.data as Data\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F    \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from bayes_opt import BayesianOptimization\n",
    "import time\n",
    "import os\n",
    "import joblib  \n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "class MAPELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MAPELoss, self).__init__()\n",
    "    def forward(self, output, target):\n",
    "        eps = 1e-8\n",
    "        loss = torch.mean(torch.abs((target - output) / (target + eps)))\n",
    "        return loss\n",
    "\n",
    "\n",
    "class Net(nn.Module):  \n",
    "    def __init__(self, n_feature = 11, n_hidden = 64, n_output = 1, w = 6):\n",
    "        super(Net, self).__init__()    \n",
    "        self.hidden1 = torch.nn.Linear(n_feature, n_hidden) \n",
    "        nn.init.kaiming_normal_(self.hidden1.weight)\n",
    "        self.hiddens = nn.ModuleList ([nn.Linear(n_hidden, n_hidden) for i in range(w)])                            \n",
    "        for m in self.hiddens:\n",
    "            nn.init.kaiming_normal_(m.weight)   \n",
    "        self.predict = torch.nn.Linear(n_hidden, n_output)  \n",
    "        nn.init.kaiming_normal_(self.predict.weight)\n",
    "    def forward(self, x):  \n",
    "        x = self.hidden1(x)\n",
    "        x = F.relu(x)   \n",
    "        for m in self.hiddens:\n",
    "            x = m(x)\n",
    "            x = F.relu(x) \n",
    "        x = self.predict(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def train(net, num_epochs, batch_size, train_features, test_features, train_labels, test_labels,\n",
    "          train_loader, optimizer):\n",
    "    print (\"\\n=== train begin ===\")\n",
    "    train_ls, test_ls = [], []\n",
    "    loss = MAPELoss() \n",
    "    device = next(net.parameters()).device  \n",
    "    for epoch in range(num_epochs):\n",
    "        for x, y in train_loader:\n",
    "            \n",
    "            x, y = x.to(device), y.to(device)\n",
    "            ls = loss(net(x).view(-1, 1), y.view(-1, 1))\n",
    "            optimizer.zero_grad()\n",
    "            ls.backward()\n",
    "            optimizer.step()\n",
    "        if epoch % 100 == 0:\n",
    "            \n",
    "            train_feat, train_lab = train_features.to(device), train_labels.to(device)\n",
    "            test_feat, test_lab = test_features.to(device), test_labels.to(device)\n",
    "            train_ls.append(loss(net(train_feat).view(-1, 1), train_lab.view(-1, 1)).item())\n",
    "            test_ls.append(loss(net(test_feat).view(-1, 1), test_lab.view(-1, 1)).item())\n",
    "            print (f\"epoch {epoch}: train loss {train_ls[-1]:.4f}, test loss {test_ls[-1]:.4f}\")\n",
    "    print (\"=== train end ===\")\n",
    "\n",
    "\n",
    "def test(model, test_loader, set_name=\"Test set\"):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    n = 0\n",
    "    loss = MAPELoss() \n",
    "    device = next(model.parameters()).device  \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            \n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += loss(output.view(-1, 1), target.view(-1, 1)).item()  \n",
    "            n += 1\n",
    "    test_loss /= n\n",
    "    print(f'{set_name}: Average loss: {test_loss:.4f}')\n",
    "    return test_loss\n",
    "\n",
    "\n",
    "def load_normalized_data_excel(load_path='normalized_data_excel'):\n",
    "    \n",
    "    \n",
    "    all_feat_cols = ['Ba', 'Ca', 'Sr', 'Ti', 'Zr', 'Sn', 'Hf', 'W', 'EI', 'EA', 'Î¼']\n",
    "    label_col = 'd33(pC/N)'\n",
    "\n",
    "    \n",
    "    train_df = pd.read_excel(f'{load_path}/train_set.xlsx', engine='openpyxl')\n",
    "    test_df = pd.read_excel(f'{load_path}/test_set.xlsx', engine='openpyxl')\n",
    "\n",
    "    \n",
    "    train_features = torch.FloatTensor(train_df[all_feat_cols].values)\n",
    "    train_labels = torch.FloatTensor(train_df[[label_col]].values)\n",
    "    test_features = torch.FloatTensor(test_df[all_feat_cols].values)\n",
    "    test_labels = torch.FloatTensor(test_df[[label_col]].values)\n",
    "\n",
    "    print(f\"\\nâœ… æˆåŠŸåŠ è½½Excelè®­ç»ƒ/æµ‹è¯•é›†\")\n",
    "    print(f\"è®­ç»ƒé›†ï¼šç‰¹å¾{train_features.shape} | æ ‡ç­¾{train_labels.shape}\")\n",
    "    print(f\"æµ‹è¯•é›†ï¼šç‰¹å¾{test_features.shape} | æ ‡ç­¾{test_labels.shape}\")\n",
    "\n",
    "    return train_features, train_labels, test_features, test_labels\n",
    "\n",
    "\n",
    "starttime = datetime.datetime.now()\n",
    "\n",
    "output_dir = 'NNBayesian'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "\n",
    "train_features, train_labels, test_features, test_labels = load_normalized_data_excel()\n",
    "\n",
    "\n",
    "def train_model(batch_size, lr, module__n_hidden, module__w):\n",
    "    module__n_hidden = int(module__n_hidden)  \n",
    "    module__w = int(module__w)  \n",
    "    batch_size = int(batch_size)\n",
    "\n",
    "    \n",
    "    train_dataset = Data.TensorDataset(train_features, train_labels)\n",
    "    test_dataset = Data.TensorDataset(test_features, test_labels)\n",
    "    \n",
    "    train_loader = Data.DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "    test_loader = Data.DataLoader(test_dataset, batch_size, shuffle=False)\n",
    "\n",
    "    \n",
    "    net = Net(n_feature=11, n_hidden=module__n_hidden, n_output=1, w=module__w)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  \n",
    "    net.to(device)  \n",
    "\n",
    "    \n",
    "    n_epochs = 1000\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=0.0001)\n",
    "\n",
    "    \n",
    "    train(net, n_epochs, batch_size, train_features, test_features, train_labels, test_labels, train_loader, optimizer)\n",
    "    \n",
    "    \n",
    "    train_loss = test(net, train_loader, set_name=\"Training set\")  \n",
    "    test_loss = test(net, test_loader, set_name=\"Test set\")  \n",
    "\n",
    "    \n",
    "    r = -np.abs(train_loss - test_loss)\n",
    "    return -test_loss\n",
    "\n",
    "\n",
    "bounds = {'lr': (0.0001, 0.001), \n",
    "          'batch_size': (32, 64), \n",
    "          'module__n_hidden': (64, 256), \n",
    "          'module__w': (2, 10)}\n",
    "\n",
    "\n",
    "bo_optimizer = BayesianOptimization(\n",
    "    f=train_model,\n",
    "    pbounds=bounds,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "\n",
    "bo_optimizer.maximize(init_points=100, n_iter=150)  \n",
    "\n",
    "\n",
    "result_list = []  \n",
    "for res in bo_optimizer.res:\n",
    "    result_list.append(pd.DataFrame({'target': [res['target']],\n",
    "                                     'batch_size': [res['params']['batch_size']],\n",
    "                                     'lr': [res['params']['lr']],\n",
    "                                     'module__n_hidden': [res['params']['module__n_hidden']],\n",
    "                                     'module__w': [res['params']['module__w']]}))\n",
    "\n",
    "table = pd.concat(result_list, ignore_index=True)\n",
    "\n",
    "\n",
    "table = pd.concat([table, pd.DataFrame({'target': [bo_optimizer.max['target']],\n",
    "                                        'batch_size': [bo_optimizer.max['params']['batch_size']],\n",
    "                                        'lr': [bo_optimizer.max['params']['lr']],\n",
    "                                        'module__n_hidden': [bo_optimizer.max['params']['module__n_hidden']],\n",
    "                                        'module__w': [bo_optimizer.max['params']['module__w']]})], \n",
    "                                        ignore_index=True)\n",
    "\n",
    "\n",
    "model_name = 'd33_inference_NN_{}'.format(datetime.datetime.now().strftime('%Y%m%d_%H%M%S'))\n",
    "file_name = os.path.join(output_dir, '{}.xlsx'.format(model_name))\n",
    "\n",
    "\n",
    "endtime = datetime.datetime.now()\n",
    "Rtime = endtime - starttime\n",
    "print(f\"\\næ€»è¿è¡Œæ—¶é—´ï¼š{Rtime}\")\n",
    "\n",
    "\n",
    "table.to_excel(file_name, index=False, engine='openpyxl')  \n",
    "print(\"ä¿å­˜è°ƒå‚ç»“æœè‡³: \", file_name)\n",
    "print(\"\\næœ€ä¼˜è¶…å‚æ•°ï¼š\")\n",
    "print(bo_optimizer.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbdd447",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'd33_inference_NN'\n",
    "file_name = '{}.xlsx'.format(model_name)\n",
    "endtime = datetime.datetime.now()\n",
    "Rtime = endtime - starttime\n",
    "print(Rtime)\n",
    "table.to_excel(file_name)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8849c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import torch.utils.data as Data\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F   \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random  \n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import os\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import r2_score\n",
    "import joblib  \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "class MAPELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MAPELoss, self).__init__()\n",
    "    def forward(self, output, target):\n",
    "        eps = 1e-8\n",
    "        loss = torch.mean(torch.abs((target - output) / (target + eps)))\n",
    "        return loss\n",
    "\n",
    "\n",
    "def load_normalized_data_excel(load_path='normalized_data_excel'):\n",
    "    all_feat_cols = ['Ba', 'Ca', 'Sr', 'Ti', 'Zr', 'Sn', 'Hf', 'W', 'EI', 'EA', 'Î¼']\n",
    "    label_col = 'd33(pC/N)'\n",
    "    \n",
    "    train_df = pd.read_excel(f'{load_path}/train_set.xlsx', engine='openpyxl')\n",
    "    test_df = pd.read_excel(f'{load_path}/test_set.xlsx', engine='openpyxl')\n",
    "    all_df = pd.read_excel(f'{load_path}/all_set.xlsx', engine='openpyxl')\n",
    "    \n",
    "    x_train = torch.FloatTensor(train_df[all_feat_cols].values)\n",
    "    y_train = torch.FloatTensor(train_df[[label_col]].values)\n",
    "    x_test = torch.FloatTensor(test_df[all_feat_cols].values)\n",
    "    y_test = torch.FloatTensor(test_df[[label_col]].values)\n",
    "    x_all = torch.FloatTensor(all_df[all_feat_cols].values)\n",
    "    y_all = torch.FloatTensor(all_df[[label_col]].values)\n",
    "    \n",
    "    comp_scaler = joblib.load(f'{load_path}/composition_scaler.joblib')\n",
    "    desc_scaler = joblib.load(f'{load_path}/descriptors_scaler.joblib')\n",
    "    print(\"âœ… æˆåŠŸåŠ è½½å½’ä¸€åŒ–Excelæ•°æ®ï¼ˆTensoræ ¼å¼ï¼‰\")\n",
    "    print(f\"å…¨é‡ï¼š{x_all.shape} | è®­ç»ƒï¼š{x_train.shape} | æµ‹è¯•ï¼š{x_test.shape}\")\n",
    "    return x_all, y_all, x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "set_random_seed(1)\n",
    "\n",
    "folder_dir = 'Results/STU_NN_BO(100+150)_1'\n",
    "os.makedirs(folder_dir, exist_ok=True)  \n",
    "\n",
    "folder_dir_figures = os.path.join(folder_dir, 'Figures')\n",
    "os.makedirs(folder_dir_figures, exist_ok=True)  \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ğŸ”§ è®­ç»ƒè®¾å¤‡ï¼š{device}\")\n",
    "\n",
    "plt.close('all')\n",
    "\n",
    "target = pd.read_excel('d33_inference_NN.xlsx', engine='openpyxl')\n",
    "\n",
    "starttime = datetime.datetime.now()\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(columns=[\n",
    "    'Iteration', 'Seed', 'target', 'R2_Score_test', 'Figure_Path_test',\n",
    "    'R2_Score_train', 'Figure_Path_train', 'R2_Score_all', 'Figure_Path_all',\n",
    "    'Final_Train_Loss', 'Final_Test_Loss'\n",
    "])\n",
    "\n",
    "\n",
    "x_all, y_all, train_features, test_features, train_labels, test_labels = load_normalized_data_excel()\n",
    "\n",
    "x_all, y_all = x_all.to(device), y_all.to(device)\n",
    "train_features, train_labels = train_features.to(device), train_labels.to(device)\n",
    "test_features, test_labels = test_features.to(device), test_labels.to(device)\n",
    "\n",
    "train_df = pd.DataFrame(train_features.cpu().numpy())\n",
    "test_df = pd.DataFrame(test_features.cpu().numpy())\n",
    "train_file_path = os.path.join(folder_dir, 'train_features.xlsx')  \n",
    "test_file_path = os.path.join(folder_dir, 'test_features.xlsx')    \n",
    "train_df.to_excel(train_file_path, index=True, engine='openpyxl')  \n",
    "test_df.to_excel(test_file_path, index=True, engine='openpyxl')   \n",
    "print(f\"ğŸ“„ è®­ç»ƒ/æµ‹è¯•ç‰¹å¾å·²ä¿å­˜è‡³ {folder_dir}\")\n",
    "\n",
    "\n",
    "for i in range(0, 251):  \n",
    "    \n",
    "    for j in range(1, 2):  \n",
    "        set_random_seed(1)\n",
    "        print(f\"\\n===== å¼€å§‹è®­ç»ƒï¼šç¬¬{i}è½®è°ƒå‚ç»“æœ | ç§å­{j} =====\")\n",
    "\n",
    "        \n",
    "        tg = target.at[i, 'target']\n",
    "        lr = target.at[i, 'lr'] \n",
    "        module__n_hidden = target.at[i, 'module__n_hidden']\n",
    "        module__w = target.at[i, 'module__w']\n",
    "        batch_size = target.at[i, 'batch_size']\n",
    "        \n",
    "        module__n_hidden = int(module__n_hidden)\n",
    "        module__w = int(module__w)\n",
    "        batch_size = int(batch_size)\n",
    "\n",
    "        \n",
    "        train_dataset = Data.TensorDataset(train_features, train_labels)\n",
    "        test_dataset = Data.TensorDataset(test_features, test_labels) \n",
    "        train_loader = Data.DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "        test_loader = Data.DataLoader(test_dataset, batch_size, shuffle=False) \n",
    "\n",
    "        \n",
    "        class Net(nn.Module):  \n",
    "            def __init__(self, n_feature=11, n_hidden=module__n_hidden, n_output=1, w=module__w):\n",
    "                super(Net, self).__init__()   \n",
    "                self.hidden1 = nn.Linear(n_feature, n_hidden) \n",
    "                nn.init.kaiming_normal_(self.hidden1.weight)\n",
    "                self.hiddens = nn.ModuleList([nn.Linear(n_hidden, n_hidden) for _ in range(w)])                            \n",
    "                for m in self.hiddens:\n",
    "                    nn.init.kaiming_normal_(m.weight)   \n",
    "                self.dropout = nn.Dropout(p=0.01)  \n",
    "                self.predict = nn.Linear(n_hidden, n_output) \n",
    "                nn.init.kaiming_normal_(self.predict.weight)\n",
    "\n",
    "            def forward(self, x): \n",
    "                x = self.hidden1(x)\n",
    "                x = F.relu(x)   \n",
    "                x = self.dropout(x)\n",
    "                for m in self.hiddens:\n",
    "                    x = m(x)\n",
    "                    x = F.relu(x) \n",
    "                    x = self.dropout(x)          \n",
    "                x = self.predict(x)\n",
    "                return x\n",
    "\n",
    "        \n",
    "        net = Net().to(device)\n",
    "        \n",
    "        train_ls, test_ls = [], []\n",
    "        loss = MAPELoss() \n",
    "        n_epochs = 1000\n",
    "        optimizer = optim.Adam(net.parameters(), lr=lr, weight_decay=0.0001)\n",
    "        \n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            net.train()  \n",
    "            for x, y in train_loader:\n",
    "                x, y = x.to(device), y.to(device)  \n",
    "                ls = loss(net(x).view(-1, 1), y.view(-1, 1))\n",
    "                optimizer.zero_grad()\n",
    "                ls.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            net.eval()  \n",
    "            with torch.no_grad():  \n",
    "                train_loss = loss(net(train_features).view(-1, 1), train_labels.view(-1, 1)).item()\n",
    "                test_loss = loss(net(test_features).view(-1, 1), test_labels.view(-1, 1)).item()\n",
    "                train_ls.append(train_loss)\n",
    "                test_ls.append(test_loss)\n",
    "\n",
    "        \n",
    "        loss_data = pd.DataFrame({'Epoch': range(1, n_epochs + 1), 'Train Loss': train_ls, 'Test Loss': test_ls})\n",
    "        loss_file_path = os.path.join(folder_dir, f'loss_data_{i}_seed_{j}.xlsx')\n",
    "        loss_data.to_excel(loss_file_path, index=False, engine='openpyxl')\n",
    "\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(range(1, n_epochs + 1), train_ls, label=\"Train Loss\", color='blue')\n",
    "        plt.plot(range(1, n_epochs + 1), test_ls, label=\"Test Loss\", color='orange')\n",
    "        plt.xlabel('Epoch'), plt.ylabel('MAPE Loss'), plt.legend()\n",
    "        plt.title(f'Loss Curve - Iter {i} Seed {j}')\n",
    "        plt.text(60, max(train_ls)*0.7, f'Target={tg:.4f}', fontdict={'size': 12, 'color':  'red'})\n",
    "        fig_name_1 = os.path.join(folder_dir_figures, f'{i}-seed_{j}_loss.png')\n",
    "        plt.savefig(fig_name_1, format='png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()  \n",
    "\n",
    "       \n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            predict_test = net(test_features).cpu().data.numpy()\n",
    "            predict_train = net(train_features).cpu().data.numpy()\n",
    "            predict_all = net(x_all).cpu().data.numpy()\n",
    "        \n",
    "        y_test_np = test_labels.cpu().data.numpy()\n",
    "        y_train_np = train_labels.cpu().data.numpy()\n",
    "        y_all_np = y_all.cpu().data.numpy()\n",
    "\n",
    "        \n",
    "        pd.DataFrame({\n",
    "            'Predicted': predict_train.flatten(), \n",
    "            'Actual': y_train_np.flatten()\n",
    "        }).to_excel(os.path.join(folder_dir, f'predictions_train_{i}_seed_{j}.xlsx'), index=False, engine='openpyxl')\n",
    "        pd.DataFrame({\n",
    "            'Predicted': predict_test.flatten(), \n",
    "            'Actual': y_test_np.flatten()\n",
    "        }).to_excel(os.path.join(folder_dir, f'predictions_test_{i}_seed_{j}.xlsx'), index=False, engine='openpyxl')\n",
    "        pd.DataFrame({\n",
    "            'Predicted': predict_all.flatten(), \n",
    "            'Actual': y_all_np.flatten()\n",
    "        }).to_excel(os.path.join(folder_dir, f'predictions_all_{i}_seed_{j}.xlsx'), index=False, engine='openpyxl')\n",
    "\n",
    "        \n",
    "        fig_name_2_test = os.path.join(folder_dir_figures, f'{i}-seed_{j}_test.png')\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        sns.regplot(x=predict_test, y=y_test_np, color='red', line_kws={'color':'black'})\n",
    "        plt.xlabel('Predicted d33'), plt.ylabel('Actual d33'), plt.title(f'Test Set - Iter {i} Seed {j}')\n",
    "        r2_test = r2_score(y_test_np, predict_test)\n",
    "        plt.text(min(predict_test), max(y_test_np), f'R2={r2_test:.4f}', color='red', fontsize=12)\n",
    "        plt.savefig(fig_name_2_test, format='png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        \n",
    "        fig_name_2_train = os.path.join(folder_dir_figures, f'{i}-seed_{j}_train.png')\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        sns.regplot(x=predict_train, y=y_train_np, color='blue', line_kws={'color':'black'})\n",
    "        plt.xlabel('Predicted d33'), plt.ylabel('Actual d33'), plt.title(f'Train Set - Iter {i} Seed {j}')\n",
    "        r2_train = r2_score(y_train_np, predict_train)\n",
    "        plt.text(min(predict_train), max(y_train_np), f'R2={r2_train:.4f}', color='blue', fontsize=12)\n",
    "        plt.savefig(fig_name_2_train, format='png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        \n",
    "        fig_name_2_all = os.path.join(folder_dir_figures, f'{i}-seed_{j}_all.png')\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        sns.regplot(x=predict_train, y=y_train_np, color='blue', label=\"Train\", line_kws={'color':'blue'})\n",
    "        sns.regplot(x=predict_test, y=y_test_np, color='red', label=\"Test\", line_kws={'color':'red'})\n",
    "        plt.xlabel('Predicted d33'), plt.ylabel('Actual d33'), plt.title(f'All Set - Iter {i} Seed {j}')\n",
    "        r2_all = r2_score(y_all_np, predict_all)\n",
    "        plt.text(min(predict_all), max(y_all_np), f'R2={r2_all:.4f}', color='green', fontsize=12)\n",
    "        plt.legend(), plt.axis('equal')\n",
    "        plt.savefig(fig_name_2_all, format='png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        \n",
    "        final_train_loss = train_ls[-1]  \n",
    "        final_test_loss = test_ls[-1]  \n",
    "\n",
    "        \n",
    "        results_df = pd.concat([results_df, pd.DataFrame([{\n",
    "            'Iteration': i,\n",
    "            'Seed': j,\n",
    "            'target': tg,\n",
    "            'R2_Score_test': r2_test,\n",
    "            'Figure_Path_test': fig_name_2_test,\n",
    "            'R2_Score_train': r2_train,\n",
    "            'Figure_Path_train': fig_name_2_train,\n",
    "            'R2_Score_all': r2_all,\n",
    "            'Figure_Path_all': fig_name_2_all,\n",
    "            'Final_Train_Loss': final_train_loss,\n",
    "            'Final_Test_Loss': final_test_loss   \n",
    "        }])], ignore_index=True)\n",
    "\n",
    "        \n",
    "        net_name = os.path.join(folder_dir, f'{i}-seed_{j}.pt')\n",
    "        torch.save(net.state_dict(), net_name)\n",
    "        print(f\"âœ… ç¬¬{i}è½®è®­ç»ƒå®Œæˆï¼šæ¨¡å‹ä¿å­˜è‡³ {net_name}\")\n",
    "\n",
    "\n",
    "results_df.to_csv(os.path.join(folder_dir, 'results_summary_NN.csv'), index=False, encoding='utf-8')\n",
    "\n",
    "endtime = datetime.datetime.now()\n",
    "Rtime = endtime - starttime\n",
    "print(f\"\\n===== æ‰€æœ‰è®­ç»ƒå®Œæˆ =====\")\n",
    "print(f\"æ€»è®­ç»ƒè€—æ—¶ï¼š{Rtime}\")\n",
    "print(f\"æ‰€æœ‰ç»“æœå·²ä¿å­˜è‡³ï¼š{folder_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
